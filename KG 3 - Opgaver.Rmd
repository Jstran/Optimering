---
title: "Optimisation: Exercises 3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Gradient descent and step size

* Make sure that you have a working implementation of the gradient descent algorithm taking as arguments the objective function ($f$) and its gradient ($g = \nabla f$), and possibly other parameters

```{r}
backtracking_line_search <- function(a, c, r, x_k, p_k, g_k, f_k, f) {
	a_k <- a
	repeat {
		x_candidate <- x_k + a_k * p_k
		lhs <- f(x_candidate)
		rhs <- f_k + c * a_k * g_k * p_k
		if (lhs <= rhs) break
		a_k <- r * a_k
	}
	return(a_k)
}

printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")

# stopping criteria?

steepest_descent <- function(f, g, x_k, a, c, r, tol, k_max) {
	k <- 0
	#x_change <- abs(tol) + 1
	g_k <- abs(tol) + 1 # to get started
	while ((abs(g_k) > tol) && (k < k_max)) {
		f_k <- f(x_k)
		points(x_k, f_k)
		g_k <- g(x_k)
		p_k <- -g_k # steepest descent direction
		a_k <- backtracking_line_search(a, c, r, x_k, p_k, g_k, f_k, f)
		#a_k <- 1
		x_old <- x_k
		x_k <- x_k + a_k * p_k
		lines(c(x_old, x_k), c(f(x_old), f(x_k)), col = "red")
		
		k <- k + 1
		printfln("k = %d \t x_k = %.22f \t f_k = %f \t g_k = %f \t a_k = %f", k, x_k, f_k, g_k, a_k)
	}
	return(x_k)
}

```

```{r}
# configuration
a <- 1    # initial step-length
c <- 1e-4 # Sufficient Decrease Condition (SDC) Strength
r <- 0.75 # step-length reduction factor

f <- function(x) sin(x)
g <- function(x) cos(x)

plot(f, -5, 5)
x_k <- 0
tol <- 1e-4
k_max <- 90
x_solution <- steepest_descent(f, g, x_k, a, c, r, tol, k_max)
x_solution
```

* Make sure that the implementation supports the different step sizes we have discussed (fixed, backtracking and satisfying strong Wolfe conditions)
    + Discuss in the group how this can be done in practise (including identifing problems that you don't know how to solve technically)
* Discuss when to terminate the algorithm
* Your implementation must return both $x^*$ and the path ($\{x_k\}_k$) [Hint in pseudo code: `return(list(x = ..., path = ...))`]
    + Why? Use the information.

# Newton's method

Implement Newton's method, including using different step sizes (see above). Discuss what is needed as argument in addition to your implementation of gradient descent. Your implementation must return both $x^*$ and the iterates ($\{x_k\}_k$).

# Comparing the methods

* Look at the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function)
* Plot it for some values of $a$ and $b$
    + Discuss how to plot this function to get an impression of it
* Compare the methods mentioned above (gradient descent and Newton's method) for a few different choices of the Rosenbrock function ($a$ and $b$ parameters) as well as different step sizes algorithms and starting values

# Zoutendijk's theorem

Discuss Theorem 3.2 (Zoutendijk) and work through the proof (possibly fill in details).
